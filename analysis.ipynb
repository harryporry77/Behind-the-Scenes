{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Comprehensive LoRA Analysis for Whisper Audio Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import WhisperForAudioClassification, WhisperFeatureExtractor\n",
    "from nnsight import NNsight\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import analysis_utils\n",
    "import torch.nn as nn\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "fold_idx = 1\n",
    "model_name = 'openai/whisper-large-v2'\n",
    "id2label = {0: \"anger\", 1: \"happiness\", 2: \"neutral\", 3: \"sadness\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Device configuration\n",
    "device1 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device2 = torch.device(\"cuda:1\" if torch.cuda.device_count() > 1 else device1)\n",
    "device3 = torch.device(\"cuda:2\" if torch.cuda.device_count() > 2 else device1)\n",
    "\n",
    "print(f\"Using devices: {device1}, {device2}, {device3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_prep",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_prep_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_balanced_samples(dataset, num_per_class=10, label_column='labels', seed=42, return_indices=False):\n",
    "    ds = dataset.shuffle(seed=seed)\n",
    "    labels_all = ds[label_column]\n",
    "    unique_labels = sorted(set(labels_all))\n",
    "    counts = {lbl: 0 for lbl in unique_labels}\n",
    "    selected_indices = []\n",
    "\n",
    "    for idx, lbl in enumerate(labels_all):\n",
    "        if counts[lbl] < num_per_class:\n",
    "            selected_indices.append(idx)\n",
    "            counts[lbl] += 1\n",
    "        if all(counts[l] >= num_per_class for l in unique_labels):\n",
    "            break\n",
    "\n",
    "    selected_dataset = ds.select(selected_indices)\n",
    "    if return_indices:\n",
    "        return selected_dataset, counts, selected_indices\n",
    "    return selected_dataset, counts\n",
    "\n",
    "def prepare_data(examples):\n",
    "    audio_arrays = [item['array'] for item in examples['audio']]\n",
    "    labels = examples['label']\n",
    "    speakers = [path.split('/')[-1].split('_')[0] for path in [item['path'] for item in examples['audio']]]\n",
    "    return {\n",
    "        \"audio\": audio_arrays,\n",
    "        \"labels\": labels,\n",
    "        \"speaker\": speakers\n",
    "    }\n",
    "\n",
    "print(\"Loading IEMOCAP dataset...\")\n",
    "ds = load_dataset(\"Zahra99/IEMOCAP_Audio\")\n",
    "dataset = concatenate_datasets([ds['session1'], ds['session2'], ds['session3'], ds['session4'], ds['session5']])\n",
    "del ds\n",
    "\n",
    "dataset = dataset.map(prepare_data, batched=True, remove_columns=dataset.column_names, num_proc=4)\n",
    "speakers_array = np.array(dataset['speaker'])\n",
    "all_speaker = sorted(list(set(dataset['speaker'])))\n",
    "test_speaker = all_speaker[fold_idx-1]\n",
    "test_indices = np.where(speakers_array == test_speaker)[0]\n",
    "test_data = dataset.select(test_indices)\n",
    "\n",
    "# Prepare train data for gradient analysis\n",
    "train_speaker = [speaker for speaker in all_speaker if speaker != test_speaker]\n",
    "train_indices = np.where(np.isin(speakers_array, train_speaker))[0]\n",
    "train_data = dataset.select(train_indices)\n",
    "del dataset\n",
    "\n",
    "# Select balanced samples\n",
    "test_samples, counts_test = select_balanced_samples(test_data, num_per_class=10, label_column='labels', seed=42)\n",
    "train_samples, counts_train = select_balanced_samples(train_data, num_per_class=10, label_column='labels', seed=42)\n",
    "del train_data\n",
    "\n",
    "print(f\"Test samples: {len(test_data)}, Train samples for gradient analysis: {len(train_samples)}\")\n",
    "print(f\"Test speaker: {test_speaker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processor_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperFeatureExtractor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_utils_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(save_path):\n",
    "    checkpoint = os.path.join(\n",
    "        save_path,\n",
    "        [d for d in os.listdir(save_path) if os.path.isdir(os.path.join(save_path, d)) and d.startswith('checkpoint-')][0]\n",
    "    )\n",
    "    model = WhisperForAudioClassification.from_pretrained(\n",
    "        checkpoint,\n",
    "        num_labels=4,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    model.eval()\n",
    "    return NNsight(model)\n",
    "\n",
    "def create_random_lora_model(target_modules=[\"v_proj\"], r=32):\n",
    "    base_model = WhisperForAudioClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=4,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    lora_config = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=64,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        modules_to_save=[\"projector\", \"classifier\"],\n",
    "    )\n",
    "    random_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "    for name, param in random_model.named_parameters():\n",
    "        if 'lora' in name and param.requires_grad:\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(param, mean=0.0, std=0.02)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "    return random_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exp1_title",
   "metadata": {},
   "source": [
    "# Experiment 1: LoRA Contribution Analysis\n",
    "\n",
    "This experiment analyzes the contribution of attention and MLP components across different layers in LoRA-adapted and frozen encoder models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp1_load_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading LoRA and frozen models...\")\n",
    "lora_save_path = f\"./{model_name}-lora/fold{fold_idx}\"\n",
    "frozen_save_path = f\"./frozen-encoder/{model_name.split('/')[-1]}/fold{fold_idx}\"\n",
    "\n",
    "lora_model = get_model(lora_save_path)\n",
    "frozen_model = get_model(frozen_save_path)\n",
    "\n",
    "lora_model.to(device1)\n",
    "frozen_model.to(device2)\n",
    "\n",
    "num_layers = lora_model.config.encoder_layers\n",
    "print(f\"Number of encoder layers: {num_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp1_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analyzing LoRA model contributions...\")\n",
    "rc_att, rc_mlp, rc_layer, att_cos, mlp_cos, layer_cos = analysis_utils.analyze_norms(\n",
    "    lora_model, test_data, processor, len(test_data)\n",
    ")\n",
    "\n",
    "print(\"Analyzing frozen model contributions...\")\n",
    "rc_att_frozen, rc_mlp_frozen, rc_layer_frozen, att_cos_frozen, mlp_cos_frozen, layer_cos_frozen = analysis_utils.analyze_norms(\n",
    "    frozen_model, test_data, processor, len(test_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp1_plotting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_zorder(bars):\n",
    "    for group in zip(*[container for container in bars]):\n",
    "        z = len(group)\n",
    "        for bar in sorted(group, key=lambda b: abs(b.get_height())):\n",
    "            bar.set_zorder(z)\n",
    "            z -= 1\n",
    "\n",
    "def plot_residual_stats(att, mlp, layer, title_suffix=\"\"):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    bars = []\n",
    "    bars.append(plt.bar([x for x in range(num_layers)], att.float().cpu().numpy(), \n",
    "                        label=\"Attention\", width=0.8, alpha=0.7))\n",
    "    bars.append(plt.bar([x for x in range(num_layers)], mlp.float().cpu().numpy(), \n",
    "                        label=\"MLP\", width=0.8, alpha=0.7))\n",
    "    bars.append(plt.bar([x for x in range(num_layers)], layer.float().cpu().numpy(), \n",
    "                        label=\"Attention + MLP\", width=0.8, alpha=0.7))\n",
    "    \n",
    "    plt.legend(fontsize=12)\n",
    "    sort_zorder(bars)\n",
    "    plt.xlim(-0.5, num_layers-0.5)\n",
    "    plt.xlabel(\"Layer Index\", fontsize=12)\n",
    "    plt.ylabel(\"Relative Contribution\", fontsize=12)\n",
    "    plt.title(f\"Layer-wise Contribution Analysis{title_suffix}\", fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_cosine_similarity(att_cos, mlp_cos, layer_cos, title_suffix=\"\"):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    bars = []\n",
    "    bars.append(plt.bar([x for x in range(num_layers)], att_cos.float().cpu().numpy(), \n",
    "                        label=\"Attention\", width=0.8, alpha=0.7))\n",
    "    bars.append(plt.bar([x for x in range(num_layers)], mlp_cos.float().cpu().numpy(), \n",
    "                        label=\"MLP\", width=0.8, alpha=0.7))\n",
    "    bars.append(plt.bar([x for x in range(num_layers)], layer_cos.float().cpu().numpy(), \n",
    "                        label=\"Attention + MLP\", width=0.8, alpha=0.7))\n",
    "    \n",
    "    plt.legend(fontsize=12)\n",
    "    sort_zorder(bars)\n",
    "    plt.xlim(-0.5, num_layers-0.5)\n",
    "    plt.xlabel(\"Layer Index\", fontsize=12)\n",
    "    plt.ylabel(\"Cosine Similarity\", fontsize=12)\n",
    "    plt.title(f\"Layer-wise Cosine Similarity Analysis{title_suffix}\", fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Plotting LoRA model results...\")\n",
    "plot_residual_stats(rc_att, rc_mlp, rc_layer, \" - LoRA Model\")\n",
    "plot_cosine_similarity(att_cos, mlp_cos, layer_cos, \" - LoRA Model\")\n",
    "\n",
    "print(\"Plotting frozen model results...\")\n",
    "plot_residual_stats(rc_att_frozen, rc_mlp_frozen, rc_layer_frozen, \" - Frozen Model\")\n",
    "plot_cosine_similarity(att_cos_frozen, mlp_cos_frozen, layer_cos_frozen, \" - Frozen Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10bd9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting LoRA - frozen model results...\")\n",
    "plot_residual_stats(rc_att-rc_att_frozen, rc_mlp-rc_mlp_frozen, rc_layer-rc_layer_frozen, \" - LoRA\")\n",
    "plot_cosine_similarity(att_cos-att_cos_frozen, mlp_cos-mlp_cos_frozen, layer_cos-layer_cos_frozen, \" - LoRA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0335f3",
   "metadata": {},
   "source": [
    "# Experiment 2: Logit-len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b97aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_logit_len = analysis_utils.logit_lens(lora_model,test_data)\n",
    "frozen_logit_len = analysis_utils.logit_lens(frozen_model,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade8811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kl(lora_logit_len, frozen_logit_len):\n",
    "    avg_kl_divs_lora = lora_logit_len['avg_kl_divs']\n",
    "    avg_kl_divs_frozen = frozen_logit_len['avg_kl_divs']\n",
    "    avg_overlaps_lora = lora_logit_len['avg_overlaps']\n",
    "    avg_overlaps_frozen = frozen_logit_len['avg_overlaps']\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 5))\n",
    "    layers = range(len(avg_kl_divs_lora))\n",
    "    ax1.plot(layers, avg_kl_divs_lora, 'b-o', linewidth=2, markersize=6, label='LoRA-encoder', alpha=0.8)        \n",
    "    ax1.plot(layers, avg_kl_divs_frozen, 'r-s', linewidth=2, markersize=6, label='Frozen-encoder', alpha=0.8)\n",
    "    ax1.set_ylabel('Average KL Divergence')      \n",
    "    ax1.set_title('Layer-wise KL Divergence from Final Prediction')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(layers, avg_overlaps_lora, 'b-o', linewidth=2, markersize=6, label='LoRA-encoder', alpha=0.8)        \n",
    "    ax2.plot(layers, avg_overlaps_frozen, 'r-s', linewidth=2, markersize=6, label='Frozen-encoder', alpha=0.8)\n",
    "    ax2.set_xlabel('Layer Index')\n",
    "    ax2.set_ylabel('Average Overlap')      \n",
    "    ax2.set_title('Layer-wise Prediction Consistency with Final Decision')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_kl(lora_logit_len,frozen_logit_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exp2_title",
   "metadata": {},
   "source": [
    "# Experiment 3: Rank Comparison with t-SNE\n",
    "\n",
    "This experiment compares different LoRA ranks (8, 32, 64) using t-SNE visualization of the rank space activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp2_load_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading models with different LoRA ranks...\")\n",
    "lora64_save_path = f\"./ablation-large-v2/rank64_modulesv_proj/fold{fold_idx}\"\n",
    "lora32_save_path = f\"./ablation-large-v2/rank32_modulesv_proj/fold{fold_idx}\"\n",
    "lora8_save_path = f\"./ablation-large-v2/rank8_modulesv_proj/fold{fold_idx}\"\n",
    "\n",
    "lora64_model = get_model(lora64_save_path)\n",
    "lora32_model = get_model(lora32_save_path)\n",
    "lora8_model = get_model(lora8_save_path)\n",
    "\n",
    "lora8_model.to(device1)\n",
    "lora32_model.to(device2)\n",
    "lora64_model.to(device3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp2_extract_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define layers for analysis (selected layers)\n",
    "selected_layers = [0, 7, 15, 23, 31]\n",
    "lora_layers = [f\"encoder.layers.{i}.self_attn.v_proj\" for i in selected_layers]\n",
    "\n",
    "def get_rank_activations(nmodel, test_data, lora_layers):\n",
    "    device = nmodel.device\n",
    "    all_rank = []\n",
    "    \n",
    "    for sample_idx, sample in tqdm(enumerate(test_data), desc=\"Extracting activations\"):\n",
    "        input_features = processor(\n",
    "            sample['audio'],\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features.to(device)\n",
    "        \n",
    "        rank = []\n",
    "        with torch.no_grad():\n",
    "            with nmodel.trace(input_features) as tracer:\n",
    "                for layer_name in lora_layers:\n",
    "                    activation = nmodel.get(f'{layer_name}.lora_A.default.output').clone().detach().cpu().save()\n",
    "                    rank.append(activation)\n",
    "        \n",
    "        rank = torch.stack(rank, dim=0)\n",
    "        all_rank.append(rank)\n",
    "    \n",
    "    return torch.stack(all_rank, dim=0)\n",
    "\n",
    "# Extract rank activations\n",
    "print(\"Extracting rank-8 activations...\")\n",
    "lora8_rank = get_rank_activations(lora8_model, test_data, lora_layers)\n",
    "\n",
    "print(\"Extracting rank-32 activations...\")\n",
    "lora32_rank = get_rank_activations(lora32_model, test_data, lora_layers)\n",
    "\n",
    "print(\"Extracting rank-64 activations...\")\n",
    "lora64_rank = get_rank_activations(lora64_model, test_data, lora_layers)\n",
    "\n",
    "print(f\"Rank shapes - 8: {lora8_rank.shape}, 32: {lora32_rank.shape}, 64: {lora64_rank.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a1971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_layers = [0, 7, 15, 23, 31]\n",
    "lora_layers = [f\"encoder.layers.{i}.self_attn.v_proj\" for i in selected_layers]\n",
    "\n",
    "def get_rank_activations(nmodel, test_data, lora_layers):\n",
    "    device = nmodel.device\n",
    "    all_rank = []\n",
    "    \n",
    "    for sample_idx, sample in tqdm(enumerate(test_data), desc=\"Extracting activations\"):\n",
    "        input_features = processor(\n",
    "            sample['audio'],\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features.to(device)\n",
    "        \n",
    "        rank = []\n",
    "        with torch.no_grad():\n",
    "            with nmodel.trace(input_features) as tracer:\n",
    "                for layer_name in lora_layers:\n",
    "                    activation = nmodel.get(f'{layer_name}.lora_A.default.output').clone().detach().cpu().save()\n",
    "                    rank.append(activation)\n",
    "        \n",
    "        rank = torch.stack(rank, dim=0)\n",
    "        all_rank.append(rank)\n",
    "    \n",
    "    return torch.stack(all_rank, dim=0)\n",
    "\n",
    "print(\"Extracting rank-8 activations...\")\n",
    "lora8_rank = get_rank_activations(lora8_model, test_data, lora_layers)\n",
    "\n",
    "print(\"Extracting rank-32 activations...\")\n",
    "lora32_rank = get_rank_activations(lora32_model, test_data, lora_layers)\n",
    "\n",
    "print(\"Extracting rank-64 activations...\")\n",
    "lora64_rank = get_rank_activations(lora64_model, test_data, lora_layers)\n",
    "\n",
    "print(f\"Rank shapes - 8: {lora8_rank.shape}, 32: {lora32_rank.shape}, 64: {lora64_rank.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp2_tsne_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perform_tsne_analysis(rank_data_dict, test_data, selected_layers):\n",
    "    print(\"Performing t-SNE analysis...\")\n",
    "    \n",
    "\n",
    "    labels = [sample['labels'] for sample in test_data]\n",
    "    emotion_names = ['Anger', 'Happiness', 'Neutral', 'Sadness']\n",
    "    colors = ['red', 'orange', 'blue', 'purple']\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 5, figsize=(25, 15))\n",
    "    fig.suptitle('t-SNE Visualization of LoRA Rank Activations Across Layers', fontsize=16)\n",
    "    \n",
    "    rank_names = ['Rank-8', 'Rank-32', 'Rank-64']\n",
    "    \n",
    "    for rank_idx, (rank_name, rank_data) in enumerate(rank_data_dict.items()):\n",
    "        print(f\"Processing {rank_name}...\")\n",
    "        \n",
    "        for layer_idx, layer_num in enumerate(selected_layers):\n",
    "            print(f\"  Processing Layer {layer_num}...\")\n",
    "            \n",
    "\n",
    "            layer_data = rank_data[:, layer_idx, 0, :, :].numpy()  # [samples, seq_len, rank]\n",
    "\n",
    "            layer_features_flat = layer_data.reshape(layer_data.shape[0], -1)  # [samples, seq_len*rank]\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            scaler = StandardScaler()\n",
    "            layer_features_scaled = scaler.fit_transform(layer_features_flat)\n",
    "            \n",
    "            tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
    "            tsne_result = tsne.fit_transform(layer_features_scaled)\n",
    "            \n",
    "            ax = axes[rank_idx, layer_idx]\n",
    "            \n",
    "            for emotion_idx, (emotion_name, color) in enumerate(zip(emotion_names, colors)):\n",
    "                mask = np.array(labels) == emotion_idx\n",
    "                ax.scatter(tsne_result[mask, 0], tsne_result[mask, 1], \n",
    "                          c=color, label=emotion_name, alpha=0.6, s=20)\n",
    "            \n",
    "            ax.set_title(f'{rank_name} - Layer {layer_num}', fontsize=12)\n",
    "            ax.set_xlabel('t-SNE 1', fontsize=10)\n",
    "            ax.set_ylabel('t-SNE 2', fontsize=10)\n",
    "            \n",
    "            if layer_idx == 0:\n",
    "                ax.legend(fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lora_tsne_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"t-SNE analysis completed and saved as 'lora_tsne_comparison.png'\")\n",
    "\n",
    "# Perform t-SNE analysis\n",
    "rank_data_dict = {\n",
    "    'Rank-8': lora8_rank,\n",
    "    'Rank-32': lora32_rank,\n",
    "    'Rank-64': lora64_rank\n",
    "}\n",
    "\n",
    "perform_tsne_analysis(rank_data_dict, test_data, selected_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exp3_title",
   "metadata": {},
   "source": [
    "# Experiment 3: Gradient Analysis\n",
    "\n",
    "This experiment compares gradients between learned LoRA weights and randomly initialized LoRA weights to understand learning dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp3_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up gradient analysis models...\")\n",
    "lora_save_path = f\"./ablation-large-v2/rank32_modulesv_proj/fold{fold_idx}\"\n",
    "learned_model = get_model(lora_save_path)\n",
    "random_model = NNsight(create_random_lora_model(target_modules=[\"v_proj\"], r=32))\n",
    "\n",
    "# Move to devices\n",
    "learned_model.to(device1)\n",
    "random_model.to(device2)\n",
    "\n",
    "gradient_lora_layers = [f\"encoder.layers.{i}.self_attn.v_proj\" \n",
    "                       for i in range(learned_model.config.encoder_layers)]\n",
    "\n",
    "for param in random_model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in learned_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for layer in gradient_lora_layers:\n",
    "    learned_model.get(f'{layer}.lora_A.default.weight').requires_grad = True\n",
    "    learned_model.get(f'{layer}.lora_B.default.weight').requires_grad = True\n",
    "    random_model.get(f'{layer}.lora_A.default.weight').requires_grad = True\n",
    "    random_model.get(f'{layer}.lora_B.default.weight').requires_grad = True\n",
    "\n",
    "print(f\"Gradient analysis setup complete for {len(gradient_lora_layers)} layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp3_gradient_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_batch(nmodel, samples_list, lora_layers, batch_size=1):\n",
    "    device = nmodel.device\n",
    "    all_rank = {layer: [] for layer in lora_layers}\n",
    "    all_lora = {layer: [] for layer in lora_layers}\n",
    "    all_rank_grad = {layer: [] for layer in lora_layers}\n",
    "    all_lora_grad = {layer: [] for layer in lora_layers}\n",
    "    all_outputs = []\n",
    "    \n",
    "    num_samples = len(samples_list)\n",
    "    for i in tqdm(range(0, num_samples, batch_size), desc=\"Computing gradients\"):\n",
    "        batch_samples = samples_list.select(range(i, min(i + batch_size, num_samples)))\n",
    "        \n",
    "        input_audios = [sample['audio'] for sample in batch_samples]\n",
    "        labels = torch.tensor([sample['labels'] for sample in batch_samples]).to(device)\n",
    "        input_features = processor(input_audios, sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
    "        \n",
    "        rank, lora = {}, {}\n",
    "        \n",
    "        with nmodel.trace(input_features) as tracer:\n",
    "            for layer_name in lora_layers:\n",
    "                nmodel.get(f'{layer_name}.lora_A.default.output').retain_grad()\n",
    "                rank[layer_name] = nmodel.get(f'{layer_name}.lora_A.default.output').save()\n",
    "                nmodel.get(f'{layer_name}.lora_B.default.output').retain_grad()\n",
    "                lora[layer_name] = nmodel.get(f'{layer_name}.lora_B.default.output').save()\n",
    "            \n",
    "            output = nmodel.output[0].save()\n",
    "            loss = nn.CrossEntropyLoss()(output, labels).save()\n",
    "            loss.backward()\n",
    "        \n",
    "        for layer_name in lora_layers:\n",
    "            all_rank[layer_name].append(rank[layer_name].cpu())\n",
    "            all_lora[layer_name].append(lora[layer_name].cpu())\n",
    "            all_rank_grad[layer_name].append(rank[layer_name].grad.cpu())\n",
    "            all_lora_grad[layer_name].append(lora[layer_name].grad.cpu())\n",
    "        \n",
    "        all_outputs.append(output.cpu())\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    final_rank = {}\n",
    "    final_lora = {}\n",
    "    final_rank_grad = {}\n",
    "    final_lora_grad = {}\n",
    "    \n",
    "    for layer_name in lora_layers:\n",
    "        final_rank[layer_name] = torch.cat(all_rank[layer_name], dim=0)\n",
    "        final_lora[layer_name] = torch.cat(all_lora[layer_name], dim=0)\n",
    "        final_rank_grad[layer_name] = torch.cat(all_rank_grad[layer_name], dim=0)\n",
    "        final_lora_grad[layer_name] = torch.cat(all_lora_grad[layer_name], dim=0)\n",
    "    \n",
    "    final_outputs = torch.cat(all_outputs, dim=0)\n",
    "    return final_rank, final_lora, final_rank_grad, final_lora_grad, final_outputs\n",
    "\n",
    "def svd_for_tensor_dict(tensor_dict):\n",
    "    svd_results = {}\n",
    "    for layer_name, tensor in tensor_dict.items():\n",
    "        mat = tensor.reshape(-1, tensor.shape[-1]).cpu()\n",
    "        U, S, Vh = torch.linalg.svd(mat, full_matrices=False)\n",
    "        svd_results[layer_name] = S\n",
    "    return svd_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp3_compute_gradients",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing gradients for learned model...\")\n",
    "learned_rank, learned_lora, learned_rank_grad, learned_lora_grad, learned_output = get_gradient_batch(\n",
    "    learned_model, train_samples, gradient_lora_layers\n",
    ")\n",
    "\n",
    "print(\"Computing gradients for random model...\")\n",
    "random_rank, random_lora, random_rank_grad, random_lora_grad, random_output = get_gradient_batch(\n",
    "    random_model, train_samples, gradient_lora_layers\n",
    ")\n",
    "\n",
    "print(\"Computing SVD analysis...\")\n",
    "learned_svd = svd_for_tensor_dict(learned_rank)\n",
    "random_svd = svd_for_tensor_dict(random_rank)\n",
    "learned_grad_svd = svd_for_tensor_dict(learned_rank_grad)\n",
    "random_grad_svd = svd_for_tensor_dict(random_rank_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad6d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_svd_lora = svd_for_tensor_dict(learned_lora)\n",
    "random_svd_lora = svd_for_tensor_dict(random_lora)\n",
    "learned_grad_svd_lora = svd_for_tensor_dict(learned_lora_grad)\n",
    "random_grad_svd_lora = svd_for_tensor_dict(random_lora_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a2e141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def plot_four_svd_subplots_compact(learned_svd, learning_svd, grad_svd_learned, grad_svd_random,\n",
    "                                  learned_svd_lora, learning_svd_lora, grad_learned_svd_lora, \n",
    "                                  grad_learning_svd_lora, figsize=(16, 12), save_path='four_svd_comparison_compact.png'):\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    \n",
    "    subplot_configs = [{\n",
    "        'data': (learned_svd, learning_svd),\n",
    "        'title': 'LoRA A - Activation Values',\n",
    "        'ax': axes[0, 0], \n",
    "        'position': (0, 0)\n",
    "    }, {\n",
    "        'data': (grad_svd_learned, grad_svd_random),\n",
    "        'title': 'LoRA A - Gradient Values',\n",
    "        'ax': axes[0, 1],\n",
    "        'position': (0, 1)\n",
    "    }, {\n",
    "        'data': (learned_svd_lora, learning_svd_lora), \n",
    "        'title': 'LoRA B - Activation Values',\n",
    "        'ax': axes[1, 0],\n",
    "        'position': (1, 0)\n",
    "    }, {\n",
    "        'data': (grad_learned_svd_lora, grad_learning_svd_lora),\n",
    "        'title': 'LoRA B - Gradient Values', \n",
    "        'ax': axes[1, 1],\n",
    "        'position': (1, 1)\n",
    "    }]\n",
    "    colors = {'lora': '#2E86AB', 'random': '#A23B72'}\n",
    "    \n",
    "    def process_single_subplot(svd_results_lora, svd_results_random, ax, title, position):\n",
    "        \"\"\"处理单个子图的绘制\"\"\"\n",
    "        row, col = position\n",
    "        \n",
    "        def process_svd_results(results, color, label_prefix, linestyle):\n",
    "            energies = []\n",
    "            for i, (_, S) in enumerate(results.items()):\n",
    "                S = torch.tensor(S) if not isinstance(S, torch.Tensor) else S\n",
    "                energy = (S**2).cumsum(dim=0) / (S**2).sum()\n",
    "                energy = energy.detach().cpu().numpy()\n",
    "                energies.append(energy)\n",
    "                \n",
    "                label = f'{label_prefix}' if i == 0 else None\n",
    "                ax.plot(range(1, len(energy)+1), energy,\n",
    "                    color=color, alpha=0.4, linewidth=1.0,\n",
    "                    linestyle=linestyle, label=label)\n",
    "            return energies\n",
    "        \n",
    "        def plot_mean_curve(energies, color, label, linestyle):\n",
    "            if energies:\n",
    "                min_len = min(len(e) for e in energies)\n",
    "                mean = np.mean([e[:min_len] for e in energies], axis=0)\n",
    "                ax.plot(range(1, len(mean)+1), mean,\n",
    "                       color=color, linewidth=3.5, alpha=1.0,\n",
    "                       label=f'{label} (Mean)', linestyle=linestyle,\n",
    "                       solid_capstyle='round')\n",
    "                return mean\n",
    "            return None\n",
    "        \n",
    "        all_lora_energies = process_svd_results(svd_results_lora, colors['lora'], 'Trained LoRA', '-')\n",
    "        all_random_energies = process_svd_results(svd_results_random, colors['random'], 'Untrained LoRA', '--')\n",
    "        \n",
    "        lora_mean = plot_mean_curve(all_lora_energies, colors['lora'], 'Trained LoRA', '-')\n",
    "        random_mean = plot_mean_curve(all_random_energies, colors['random'], 'Untrained LoRA', '--')\n",
    "        \n",
    "        ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)\n",
    "        ax.set_ylim(0, 1.02)\n",
    "        ax.tick_params(axis='both', labelsize=11)\n",
    "        \n",
    "        if row == 0: \n",
    "            ax.set_xticks(list(range(0, 33, 8)))\n",
    "            ax.set_xticklabels([])\n",
    "        else:  \n",
    "            ax.set_xticks(list(range(0, 33, 8)))\n",
    "        \n",
    "        if col == 1:  \n",
    "            ax.set_yticks([0.5, 0.8, 1.0])\n",
    "            ax.set_yticklabels([])\n",
    "        else:  \n",
    "            ax.set_yticks([0.5, 0.8, 1.0])\n",
    "            ax.yaxis.set_major_formatter(plt.FormatStrFormatter('%.1f'))\n",
    "        \n",
    "        ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.8)\n",
    "        for y in [0.5, 0.8]:\n",
    "            ax.axhline(y=y, color='gray', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        ax.set_xlim(0, 32)\n",
    "        \n",
    "        def get_sv_90(energies):\n",
    "            return [np.where(e >= 0.9)[0][0] + 1 for e in energies if len(np.where(e >= 0.9)[0]) > 0]\n",
    "        \n",
    "        lora_sv_90 = get_sv_90(all_lora_energies)\n",
    "        random_sv_90 = get_sv_90(all_random_energies)\n",
    "        \n",
    "        ax_stats = ax.inset_axes([0.5, 0.0, 0.5, 0.5])\n",
    "        box_data = [lora_sv_90, random_sv_90]\n",
    "        \n",
    "        bp = ax_stats.boxplot(box_data, tick_labels=['Trained', 'Untrained'], patch_artist=True,\n",
    "                             boxprops=dict(linewidth=1.0),\n",
    "                             medianprops=dict(color='black', linewidth=1.5),\n",
    "                             whiskerprops=dict(linewidth=1.0),\n",
    "                             capprops=dict(linewidth=1.0),\n",
    "                             widths=0.5,\n",
    "                             positions=[0.7, 1.7])  \n",
    "        \n",
    "        for i, color in enumerate([colors['lora'], colors['random']]):\n",
    "            bp['boxes'][i].set_facecolor(color)\n",
    "            bp['boxes'][i].set_alpha(0.7)\n",
    "        \n",
    "        if lora_sv_90 and random_sv_90:\n",
    "            y_min = min(min(lora_sv_90), min(random_sv_90))\n",
    "            y_max = max(max(lora_sv_90), max(random_sv_90))\n",
    "            padding = (y_max - y_min) * 0.15\n",
    "            ax_stats.set_ylim(y_min - padding, y_max + padding)\n",
    "        \n",
    "        ax_stats.set_xlim(0.2, 2.2)\n",
    "        ax_stats.tick_params(labelsize=7)\n",
    "        ax_stats.grid(False)\n",
    "        ax_stats.yaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "        \n",
    "        if lora_sv_90 and random_sv_90:\n",
    "            stats = {\n",
    "                'lora_mean': np.mean(lora_sv_90),\n",
    "                'random_mean': np.mean(random_sv_90)\n",
    "            }\n",
    "            stats_text = (f\"Trained: {stats['lora_mean']:.1f}\\n\"\n",
    "                         f\"Untrained: {stats['random_mean']:.1f}\\n\"\n",
    "                         f\"Ratio: {stats['random_mean']/stats['lora_mean']:.1f}×\")\n",
    "            if position == (1, 1): \n",
    "                ax_stats.text(0.98, 0.98, stats_text, transform=ax_stats.transAxes,\n",
    "                            fontsize=12, verticalalignment='top', \n",
    "                            horizontalalignment='right', fontweight='bold',\n",
    "                            bbox=dict(boxstyle='round,pad=0.2', facecolor='white',\n",
    "                                    alpha=0.9, edgecolor='gray'))\n",
    "            else:  \n",
    "                ax_stats.text(0.02, 0.98, stats_text, transform=ax_stats.transAxes,\n",
    "                            fontsize=12, verticalalignment='top',\n",
    "                            horizontalalignment='left', fontweight='bold', \n",
    "                            bbox=dict(boxstyle='round,pad=0.2', facecolor='white',\n",
    "                                    alpha=0.9, edgecolor='gray'))\n",
    "    \n",
    "    for config in subplot_configs:\n",
    "        svd_lora, svd_random = config['data']\n",
    "        process_single_subplot(svd_lora, svd_random, config['ax'], config['title'], config['position'])\n",
    "    \n",
    "    handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "    \n",
    "    legend_fig = plt.figure(figsize=(8, 1))\n",
    "    legend_fig.legend(handles, labels, loc='center', ncol=1, \n",
    "                     fontsize=11, framealpha=0.95, fancybox=True, \n",
    "                     shadow=True, title_fontsize=12)\n",
    "    \n",
    "    legend_fig.savefig('legend.png', dpi=300, bbox_inches='tight', \n",
    "                      facecolor='white', edgecolor='none')\n",
    "    plt.close(legend_fig)\n",
    "    \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(left=0.08, bottom=0.08, top=0.95, right=0.95, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "    plt.show()\n",
    "    \n",
    "    return fig, axes\n",
    "\n",
    "fig, axes = plot_four_svd_subplots_compact(\n",
    "    learned_svd, random_svd,\n",
    "    learned_grad_svd, random_grad_svd, \n",
    "    learned_svd_lora, random_svd_lora,\n",
    "    learned_grad_svd, random_grad_svd,\n",
    "    save_path='four_svd_comparison_compact.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba91de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_layers = [0, 7, 15, 23, 31]\n",
    "lora_layers = [f\"encoder.layers.{i}.self_attn.v_proj\" for i in selected_layers]\n",
    "\n",
    "def get_rank_activations(nmodel, test_data, lora_layers):\n",
    "    device = nmodel.device\n",
    "    all_rank = []\n",
    "    \n",
    "    for sample_idx, sample in tqdm(enumerate(test_data), desc=\"Extracting activations\"):\n",
    "        input_features = processor(\n",
    "            sample['audio'],\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features.to(device)\n",
    "        \n",
    "        rank = []\n",
    "        with torch.no_grad():\n",
    "            with nmodel.trace(input_features) as tracer:\n",
    "                for layer_name in lora_layers:\n",
    "                    activation = nmodel.get(f'{layer_name}.lora_A.default.output').clone().detach().cpu().save()\n",
    "                    rank.append(activation)\n",
    "        \n",
    "        rank = torch.stack(rank, dim=0)\n",
    "        all_rank.append(rank)\n",
    "    \n",
    "    return torch.stack(all_rank, dim=0)\n",
    "\n",
    "# Extract rank activations\n",
    "print(\"Extracting rank-8 activations...\")\n",
    "lora8_rank = get_rank_activations(lora8_model, test_data, lora_layers)\n",
    "\n",
    "print(\"Extracting rank-32 activations...\")\n",
    "lora32_rank = get_rank_activations(lora32_model, test_data, lora_layers)\n",
    "\n",
    "print(\"Extracting rank-64 activations...\")\n",
    "lora64_rank = get_rank_activations(lora64_model, test_data, lora_layers)\n",
    "\n",
    "print(f\"Rank shapes - 8: {lora8_rank.shape}, 32: {lora32_rank.shape}, 64: {lora64_rank.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db4791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_kernel_matrix_torch(K):\n",
    "    n = K.shape[0]\n",
    "    unit = torch.ones(n, n, device=K.device, dtype=K.dtype) / n\n",
    "    return K - unit @ K - K @ unit + unit @ K @ unit\n",
    "\n",
    "def center_kernel_matrix_numpy(K):\n",
    "    n = K.shape[0]\n",
    "    row_mean = K.mean(axis=1, keepdims=True)\n",
    "    col_mean = K.mean(axis=0, keepdims=True) \n",
    "    total_mean = K.mean()\n",
    "    return K - row_mean - col_mean + total_mean\n",
    "\n",
    "def linear_cka_torch(X, Y):\n",
    "    if X.device != Y.device:\n",
    "        Y = Y.to(X.device)\n",
    "    \n",
    "    K = X @ X.T\n",
    "    L = Y @ Y.T\n",
    "    \n",
    "    K_centered = center_kernel_matrix_torch(K)\n",
    "    L_centered = center_kernel_matrix_torch(L)\n",
    "    \n",
    "    hsic = torch.sum(K_centered * L_centered)\n",
    "    norm_k = torch.sqrt(torch.sum(K_centered * K_centered))\n",
    "    norm_l = torch.sqrt(torch.sum(L_centered * L_centered))\n",
    "    \n",
    "    if norm_k == 0 or norm_l == 0:\n",
    "        return torch.tensor(0.0, device=X.device)\n",
    "    \n",
    "    return (hsic / (norm_k * norm_l)).cpu().item()\n",
    "\n",
    "def linear_cka_numpy(X, Y):\n",
    "    K = X @ X.T\n",
    "    L = Y @ Y.T\n",
    "    \n",
    "    K_centered = center_kernel_matrix_numpy(K)\n",
    "    L_centered = center_kernel_matrix_numpy(L)\n",
    "    hsic = np.sum(K_centered * L_centered)\n",
    "    norm_k = np.sqrt(np.sum(K_centered * K_centered))\n",
    "    norm_l = np.sqrt(np.sum(L_centered * L_centered))\n",
    "    \n",
    "    if norm_k == 0 or norm_l == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return hsic / (norm_k * norm_l)\n",
    "\n",
    "def cka_rbf_numpy(X, Y):\n",
    "    K_X = rbf_kernel(X)\n",
    "    K_Y = rbf_kernel(Y)\n",
    "    \n",
    "    K_X_centered = center_kernel_matrix_numpy(K_X)\n",
    "    K_Y_centered = center_kernel_matrix_numpy(K_Y)\n",
    "    hsic = np.sum(K_X_centered * K_Y_centered)\n",
    "    \n",
    "    norm_x = np.sqrt(np.sum(K_X_centered * K_X_centered))\n",
    "    norm_y = np.sqrt(np.sum(K_Y_centered * K_Y_centered))\n",
    "    \n",
    "    if norm_x == 0 or norm_y == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return hsic / (norm_x * norm_y)\n",
    "\n",
    "def cka(X, Y, kernel_type='linear'):\n",
    "    if isinstance(X, torch.Tensor) and isinstance(Y, torch.Tensor):\n",
    "        if kernel_type == 'linear':\n",
    "            return linear_cka_torch(X, Y)\n",
    "        else:\n",
    "            X_np = X.detach().cpu().numpy()\n",
    "            Y_np = Y.detach().cpu().numpy()\n",
    "            return cka_rbf_numpy(X_np, Y_np)\n",
    "    \n",
    "    elif isinstance(X, torch.Tensor):\n",
    "        X = X.detach().cpu().numpy()\n",
    "        \n",
    "    elif isinstance(Y, torch.Tensor):\n",
    "        Y = Y.detach().cpu().numpy()\n",
    "    \n",
    "    if kernel_type == 'linear':\n",
    "        return linear_cka_numpy(X, Y)\n",
    "    else:\n",
    "        return cka_rbf_numpy(X, Y)\n",
    "\n",
    "def layer_wise_cka_analysis(activations1, activations2, kernel_type='linear'):\n",
    "    layer_similarities = {}\n",
    "    for layer_name in activations1.keys():\n",
    "        act1 = activations1[layer_name] \n",
    "        act2 = activations2[layer_name]\n",
    "        \n",
    "        # 重塑为 (samples, features)\n",
    "        act1_reshape = act1.reshape(act1.shape[0], -1)\n",
    "        act2_reshape = act2.reshape(act2.shape[0], -1)\n",
    "        \n",
    "        similarity = cka(act1_reshape, act2_reshape, kernel_type=kernel_type)\n",
    "        layer_similarities[layer_name] = similarity\n",
    "    \n",
    "    return layer_similarities\n",
    "\n",
    "def plot_cka_comparison(learned_similarities, random_similarities):\n",
    "    layers = list(learned_similarities.keys())\n",
    "    learned_values = [learned_similarities[layer] for layer in layers]\n",
    "    random_values = [random_similarities[layer] for layer in layers]\n",
    "    \n",
    "\n",
    "    layer_indices = []\n",
    "    for layer in layers:\n",
    "        try:\n",
    "            layer_idx = int(layer.split('.')[2])\n",
    "            layer_indices.append(layer_idx)\n",
    "        except:\n",
    "            layer_indices.append(0)\n",
    "    \n",
    "    sorted_data = sorted(zip(layer_indices, learned_values, random_values))\n",
    "    layer_indices, learned_values, random_values = zip(*sorted_data)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x_pos = range(len(layers))\n",
    "    \n",
    "    plt.plot(x_pos, learned_values, 'o-', label='Trained LoRA', \n",
    "             linewidth=3, markersize=8, color='#2E86AB', alpha=0.8)\n",
    "    plt.plot(x_pos, random_values, 's--', label='Untrained LoRA', \n",
    "             linewidth=3, markersize=8, color='#F24236', alpha=0.8)\n",
    "    \n",
    "    plt.fill_between(x_pos, learned_values, random_values, \n",
    "                     alpha=0.2, color='gray')\n",
    "    \n",
    "    plt.xlabel('Layer Index', fontsize=12)\n",
    "    plt.ylabel('CKA Similarity', fontsize=12)\n",
    "    plt.title('Layer-wise CKA: Trained vs Untrained LoRA', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.xticks(x_pos[::max(1, len(x_pos)//15)], \n",
    "               [f'L{idx}' for idx in layer_indices[::max(1, len(x_pos)//15)]], \n",
    "               rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Computing CKA similarities...\")\n",
    "\n",
    "learned_layer_similarities = layer_wise_cka_analysis(\n",
    "    learned_rank, random_rank, kernel_type='linear'\n",
    ")\n",
    "random_layer_similarities = layer_wise_cka_analysis(\n",
    "    learned_lora, random_lora, kernel_type='rbf'\n",
    ")\n",
    "learned_layer_grad_similarities = layer_wise_cka_analysis(\n",
    "    learned_lora_grad, learned_rank_grad, kernel_type='rbf'\n",
    ")\n",
    "random_layer_grad_similarities = layer_wise_cka_analysis(\n",
    "    random_lora_grad, random_rank_grad, kernel_type='rbf'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe46165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_comparison_academic(learned_similarities, learning_similarities, \n",
    "                            learned_grad_similarities, learning_grad_similarities):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'serif',\n",
    "        'font.serif': ['DejaVu Serif', 'Liberation Serif', 'serif'],\n",
    "        'font.size': 10,\n",
    "        'axes.linewidth': 0.8,\n",
    "        'grid.linewidth': 0.5,\n",
    "        'lines.linewidth': 1.2,\n",
    "        'patch.linewidth': 0.5,\n",
    "        'xtick.major.width': 0.8,\n",
    "        'ytick.major.width': 0.8,\n",
    "        'xtick.minor.width': 0.6,\n",
    "        'ytick.minor.width': 0.6,\n",
    "        'axes.edgecolor': 'black',\n",
    "        'axes.grid': True,\n",
    "        'grid.alpha': 0.3\n",
    "    })\n",
    "    \n",
    "    layers = list(learned_similarities.keys())\n",
    "    learned_values = [learned_similarities[layer] for layer in layers]\n",
    "    learning_values = [learning_similarities[layer] for layer in layers]\n",
    "    learned_grad_values = [learned_grad_similarities[layer] for layer in layers]\n",
    "    learning_grad_values = [learning_grad_similarities[layer] for layer in layers]\n",
    "    \n",
    "    layer_indices = []\n",
    "    for layer in layers:\n",
    "        try:\n",
    "            layer_idx = int(layer.split('.')[2])\n",
    "            layer_indices.append(layer_idx)\n",
    "        except:\n",
    "            layer_indices.append(0)\n",
    "    \n",
    "    sorted_data = sorted(zip(layer_indices, learned_values, learning_values, \n",
    "                           learned_grad_values, learning_grad_values))\n",
    "    layer_indices, learned_values, learning_values, learned_grad_values, learning_grad_values = zip(*sorted_data)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(5, 8))\n",
    "    x_pos = np.arange(len(layers))\n",
    "    \n",
    "    ax1.plot(x_pos, learned_values, 'o-', label='Trained', \n",
    "             linewidth=1.5, markersize=3.5, color='#2E2E2E', markerfacecolor='black')\n",
    "    ax1.plot(x_pos, learning_values, 's--', label='Untrained', \n",
    "             linewidth=1.2, markersize=3.5, color='#808080', markerfacecolor='gray', alpha=0.8)\n",
    "    \n",
    "    ax1.fill_between(x_pos, learned_values, learning_values, \n",
    "                     alpha=0.12, color='lightgray')\n",
    "    \n",
    "    legend1 = ax1.legend(fontsize=10, frameon=True, fancybox=False, shadow=False, \n",
    "                        framealpha=0.9, edgecolor='black')\n",
    "    legend1.get_frame().set_linewidth(0.8)\n",
    "    ax1.grid(True, alpha=0.25, linestyle='-', linewidth=0.5)\n",
    "    ax1.set_ylim(0.82, 1.005)\n",
    "    \n",
    "    ax1.set_yticks([0.85, 0.92, 0.99])\n",
    "    \n",
    "    ax2.plot(x_pos, learned_grad_values, '^-', label='Trained', \n",
    "             linewidth=1.5, markersize=3.5, color='#2E2E2E', markerfacecolor='black')\n",
    "    ax2.plot(x_pos, learning_grad_values, 'v--', label='Untrained', \n",
    "             linewidth=1.2, markersize=3.5, color='#808080', markerfacecolor='gray', alpha=0.8)\n",
    "    \n",
    "    ax2.fill_between(x_pos, learned_grad_values, learning_grad_values, \n",
    "                     alpha=0.12, color='lightgray')\n",
    "    \n",
    "    legend2 = ax2.legend(fontsize=10, frameon=True, fancybox=False, shadow=False,\n",
    "                        framealpha=0.9, edgecolor='black')\n",
    "    legend2.get_frame().set_linewidth(0.8)\n",
    "    ax2.grid(True, alpha=0.25, linestyle='-', linewidth=0.5)\n",
    "    ax2.set_ylim(-0.8, 1.05)\n",
    "    \n",
    "    ax2.set_yticks([-0.5, 0.25, 1.0])\n",
    "    \n",
    "    # 统一设置X轴\n",
    "    for ax in [ax1, ax2]:\n",
    "        tick_positions = x_pos[::7]  \n",
    "        ax.set_xticks(tick_positions)\n",
    "        ax.set_xticklabels([f'{layer_indices[i]}' for i in tick_positions], \n",
    "                          fontsize=9)\n",
    "        ax.set_xlim(-0.5, len(layers)-0.5) \n",
    "        \n",
    "        ax.tick_params(axis='both', which='major', labelsize=9, \n",
    "                      direction='in', length=4, width=0.8)\n",
    "        ax.tick_params(axis='both', which='minor', direction='in', \n",
    "                      length=2, width=0.6)\n",
    "        \n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_linewidth(0.8)\n",
    "            spine.set_color('black')\n",
    "    \n",
    "    plt.tight_layout(pad=2.0, w_pad=3.0)\n",
    "    \n",
    "    plt.savefig('cka_academic_style.png', dpi=300, bbox_inches='tight', \n",
    "                facecolor='white', edgecolor='none')\n",
    "    plt.savefig('cka_academic_style.pdf', bbox_inches='tight', \n",
    "                facecolor='white', edgecolor='none')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    plt.rcdefaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360a0d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define layers for analysis (selected layers)\n",
    "selected_layers = [0, 7, 15, 23, 31]\n",
    "lora_layers = [f\"encoder.layers.{i}.self_attn.v_proj\" for i in selected_layers]\n",
    "\n",
    "def get_rank_activations(nmodel, test_data, lora_layers):\n",
    "    device = nmodel.device\n",
    "    all_rank = []\n",
    "    \n",
    "    for sample_idx, sample in tqdm(enumerate(test_data), desc=\"Extracting activations\"):\n",
    "        input_features = processor(\n",
    "            sample['audio'],\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features.to(device)\n",
    "        \n",
    "        rank = []\n",
    "        with torch.no_grad():\n",
    "            with nmodel.trace(input_features) as tracer:\n",
    "                for layer_name in lora_layers:\n",
    "                    activation = nmodel.get(f'{layer_name}.lora_A.default.output').clone().detach().cpu().save()\n",
    "                    rank.append(activation)\n",
    "        \n",
    "        rank = torch.stack(rank, dim=0)\n",
    "        all_rank.append(rank)\n",
    "    \n",
    "    return torch.stack(all_rank, dim=0)\n",
    "\n",
    "# Extract rank activations\n",
    "print(\"Extracting rank-8 activations...\")\n",
    "lora8_rank = get_rank_activations(lora8_model, test_data, lora_layers)\n",
    "\n",
    "print(\"Extracting rank-32 activations...\")\n",
    "lora32_rank = get_rank_activations(lora32_model, test_data, lora_layers)\n",
    "\n",
    "print(\"Extracting rank-64 activations...\")\n",
    "lora64_rank = get_rank_activations(lora64_model, test_data, lora_layers)\n",
    "\n",
    "print(f\"Rank shapes - 8: {lora8_rank.shape}, 32: {lora32_rank.shape}, 64: {lora64_rank.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_comparison_academic(learned_layer_similarities, random_layer_similarities,\n",
    "                        learned_layer_grad_similarities, random_layer_grad_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7bf3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
